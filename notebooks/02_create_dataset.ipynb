{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94a68d6-8096-4877-94e9-9d4bf8a61ce7",
   "metadata": {},
   "source": [
    "# 2. PyTorch dataset and data loader\n",
    "\n",
    "In the previous notebook, we have learned how to investigate our dataset. Next up, we'll want to create a PyTorch `Dataset` and a PyTorch `DataLoader`. These will help us get our data ready to be passed through a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d37052-c196-40d7-8f64-3c4468943576",
   "metadata": {},
   "source": [
    "## 2.1 Creating a PyTorch dataset\n",
    "\n",
    "A [PyTorch `Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is an abstract representation of a dataset. You can do two things with a PyTorch `Dataset`: get the *item* at a certain **index**, and get the **length** of the dataset (i.e., the number of *items*). That's all. What an *item* is, is entirely up to you, but in the case of classification, it is typically a tuple of an image and a class label.\n",
    "\n",
    "### 2.1.1 Built-in datasets\n",
    "\n",
    "Torchvision already includes [many datasets](https://pytorch.org/vision/stable/datasets.html) to play around with. Let's take a look at the [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ececd578-3ad8-4716-8f01-6f4a515cd45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 41494879.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 28917568.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 20686504.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist_ds = MNIST(\"../data/\",train=True,download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2a345-1ce2-4cb8-8669-f6c7317e56a1",
   "metadata": {},
   "source": [
    "We can ask an item at a certain index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc48dd3-17f3-4bb1-a784-30cca2c2a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2b470-09e4-4a6f-87ff-33d8b6936adc",
   "metadata": {},
   "source": [
    "This gives us two things: an image and an integer label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d60429-ab19-40ae-af49-caa278051341",
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = mnist_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3d3e85-c871-41c9-baa7-f3f53dedef63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c42e864-d7a8-4861-9d48-f9d6177dea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a64b-f77d-416c-8cf1-c3fa8f5bb7cd",
   "metadata": {},
   "source": [
    "We can also ask the length of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f051a95-db79-4728-8a2e-671a74b2eed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000f987-7875-4291-9a73-4c726df244e8",
   "metadata": {},
   "source": [
    "### 2.1.2 A first attempt to a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8cdd9b-7d3d-4bc6-8dc9-35a0f9f56080",
   "metadata": {},
   "source": [
    "To implement your own dataset, you need create a class that inherits from [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and implements two methods: `__getitem__()` and `__len__()`.\n",
    "\n",
    "For our Pokémon use case:\n",
    "\n",
    "- `__getitem__()` should return the image at the given index, along with the name of that Pokémon;\n",
    "- `__len__()` should return the number of images.\n",
    "\n",
    "In the previous notebook, we saw how we can represent our dataset as a Pandas `DataFrame`. We can use this representation to implement `__getitem__()` and `__len__()` without much new code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbfbeff8-cde6-49fa-a7f4-52b05c8d1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "data_path = Path(\"../data/Flowers\")\n",
    "all_files = list(data_path.glob('data/*/*'))\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "\n",
    "        'image': p,\n",
    "        'label': p.parent.name\n",
    "    }\n",
    "    for p in all_files\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d216f401-6c17-49ba-ba4c-39b995a4fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FlowersDataset(Dataset):\n",
    "    def __init__(self,datapath,transform=None):\n",
    "        data_path= Path(datapath)\n",
    "        self.df = pd.DataFrame([\n",
    "        {\n",
    "    \n",
    "            'image': p,\n",
    "            'label': p.parent.name\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for p in data_path.glob('data/*/*')\n",
    "        ])\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['image']\n",
    "        label = row['label']\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a4255-0d7d-4639-aa12-683405adf26c",
   "metadata": {},
   "source": [
    "We can play around with this dataset, just like with the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d076d02-4f1a-4be0-8108-ff2f11a27a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=272x186>, 'Aster')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = FlowersDataset(\"../data/Flowers\")\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f8bba-88e7-4227-9871-611c88d3d4bc",
   "metadata": {},
   "source": [
    "While this is already a perfectly valid PyTorch `Dataset`, there are two issues:\n",
    "\n",
    "1. Our labels are strings;\n",
    "2. We don't make a distinction between train, validation and test data.\n",
    "\n",
    "> 🤔 **Why is it a problem that our labels are strings?**\n",
    ">\n",
    "> When training a network, we typically iterate over *batches* of data. These batches are represented as PyTorch tensors. And it is not possible (and not necessary, really) to create a tensor of strings.\n",
    "\n",
    "We'll solve both issues in the following sections.\n",
    "\n",
    "### 2.1.3 Replacing string labels with integer labels\n",
    "\n",
    "First, we'll replace our string labels by integer labels. For this, we'll create a dictionary that maps each string label to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2e6dcd-2213-4819-85ff-0d2d33381d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "        {\n",
    "    \n",
    "            'image': p,\n",
    "            'label': p.parent.name\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for p in data_path.glob('data/*/*')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28016453-d16e-421f-95dc-e683d16152db",
   "metadata": {},
   "source": [
    "To get a list of unique labels, we can call [`unique()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html) on the `label` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde56d57-3c95-497f-b6c6-3af4bca7f993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aster', 'Daisy', 'Iris', 'Lavender', 'Lily', 'Marigold', 'Orchid',\n",
       "       'Poppy', 'Rose', 'Sunflower'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8294b-8e4d-4524-b150-cc97692fa7c5",
   "metadata": {},
   "source": [
    "To ensure a consistent mapping across different systems, we can sort these labels with [the built-in Python function `sorted()`](https://docs.python.org/3/library/functions.html#sorted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80660a93-a955-459c-9221-435b19542d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aster',\n",
       " 'Daisy',\n",
       " 'Iris',\n",
       " 'Lavender',\n",
       " 'Lily',\n",
       " 'Marigold',\n",
       " 'Orchid',\n",
       " 'Poppy',\n",
       " 'Rose',\n",
       " 'Sunflower']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3739f9-16da-41ef-8bc2-2ace902a8e34",
   "metadata": {},
   "source": [
    "Now, with [the built-in Python function `enumerate()`](https://docs.python.org/3/library/functions.html#enumerate), we can get an iterable that yields another integer value for each of our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708d8878-6e36-49b2-8c8c-51fe01c0f3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Aster\n",
      "1 Daisy\n",
      "2 Iris\n",
      "3 Lavender\n",
      "4 Lily\n",
      "5 Marigold\n",
      "6 Orchid\n",
      "7 Poppy\n",
      "8 Rose\n",
      "9 Sunflower\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(sorted(df['label'].unique())):\n",
    "    \n",
    "    print(i, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb35df-3a02-49c0-817b-eede45a64a5f",
   "metadata": {},
   "source": [
    "So, we can create our label-to-integer dictionary with the following `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba3371c-e5ca-4866-adbd-9e66eab8b7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_to_int = {}\n",
    "for i, label in enumerate(sorted(df['label'].unique())):\n",
    "    \n",
    "    label_to_int[label] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27170a9-ee1e-4029-8d37-5e1f80b0e59e",
   "metadata": {},
   "source": [
    "Or, we can use a [dictionary comprehension](https://www.geeksforgeeks.org/python-dictionary-comprehension/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba271583-162f-4393-bd02-e4063a035d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aster': 0,\n",
       " 'Daisy': 1,\n",
       " 'Iris': 2,\n",
       " 'Lavender': 3,\n",
       " 'Lily': 4,\n",
       " 'Marigold': 5,\n",
       " 'Orchid': 6,\n",
       " 'Poppy': 7,\n",
       " 'Rose': 8,\n",
       " 'Sunflower': 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_int = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "label_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb7c89-fd4a-438b-b95b-d529aa54adbf",
   "metadata": {},
   "source": [
    "We can plug this into our SimpsonsDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "704c42e3-c94d-4423-80d7-67fe3a7532a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowersDataset(Dataset):\n",
    "    def __init__(self,datapath,transform=None):\n",
    "        data_path= Path(datapath)\n",
    "        df = pd.DataFrame([\n",
    "        {\n",
    "    \n",
    "            'image': p,\n",
    "            'label': p.parent.name\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for p in data_path.glob('data/*/*')\n",
    "        ])\n",
    "        label_to_int = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "\n",
    "        self.df = df\n",
    "        self.label_to_int = label_to_int\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['image']\n",
    "        label = row['label']\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9383a4-23a8-497a-aba6-9814b3e07ca7",
   "metadata": {},
   "source": [
    "Let's try out our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80867fbd-c0eb-42f1-b076-2fcc5e6cee9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=272x186>, 'Aster')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = FlowersDataset(\"../data/Flowers\")\n",
    "ds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98bad5cc-d9b1-4654-8a0d-e28d8c1d40e0",
   "metadata": {},
   "source": [
    "### 2.1.4 Splitting the data into a train, validation and test split\n",
    "\n",
    "Training a neural network and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would achieve a perfect score but would fail to predict anything useful on data that was not used during training. This situation is called **overfitting**. To get a better sense of model performance on *unseen* data, it is customary to randomly split the dataset into three disjoint subsets.\n",
    "\n",
    "1. **Training set**: You train the model with the training set.\n",
    "2. **Validation set**: Every X training iterations, you evaluate the model performance on the validation set. It is common to only keep the model that obtained the best validation score during training. When training multiple models with different configurations (or *hyperparameters*), you should use the evaluation on the validation set to decide which model to keep.\n",
    "3. **Test set**: *Once you have decided a model based on validation perfomance*, you evaluate on the test set to get an estimate of the model's ability to generalize to unseen data. In many machine learning competitions, to avoid cheating, the test set is either unlabeled or unavailable to the participants.\n",
    "\n",
    "As you see, the validation set is used to compare different model configurations, or *hyperparameters*. This could simply be the number of iterations used to train the model, but this also includes the choice of data augmentations, optimizer, learning rate, model architecture (number of layers, number of neurons in a layer, kind of layer,...) etc.\n",
    "\n",
    "To split our dataset into these subsets, we can use [`sklearn.model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68764b71-5999-4d8a-b083-f37cb7c485b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62e67766-af98-497f-b02b-4ae2ccb14c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval, df_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "df_train,df_val = train_test_split(df, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65161a8-5a34-4060-9b25-d137db5e7f65",
   "metadata": {},
   "source": [
    "We can update our `SimpsonsDataset` so that, depending on a given constructor argument, the correct subset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "023a3f16-f244-46a8-a0cf-b6e433eb4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowersDataset(Dataset):\n",
    "    def __init__(self,datapath, subset, transform=None):\n",
    "        data_path= Path(datapath)\n",
    "        df = pd.DataFrame([\n",
    "        {\n",
    "    \n",
    "            'image': p,\n",
    "            'label': p.parent.name\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for p in data_path.glob('data/*/*')\n",
    "        ])\n",
    "        label_to_int = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "\n",
    "        #Train, test and val\n",
    "        df_trainval, df_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "        df_train, df_val = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "        self.df = df\n",
    "        self.label_to_int = label_to_int\n",
    "        self.transform = transform\n",
    "\n",
    "        if subset == 'train':\n",
    "            self.df = df_train.reset_index()\n",
    "        elif subset == 'val':\n",
    "            self.df = df_val.reset_index()\n",
    "        elif subset == 'test':\n",
    "            self.df = df_val.reset_index()\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['image']\n",
    "        label = row['label']\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c44e40-ed2e-44d5-9cd0-59d363450952",
   "metadata": {},
   "source": [
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89fc150f-ffe1-41ab-967c-d1e583eaea81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=225x225>, 'Orchid')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = FlowersDataset(\"../data/Flowers\",subset='train')\n",
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138eebc-b542-40eb-9487-86c59d557496",
   "metadata": {},
   "source": [
    "### 2.1.5 Cross-validation\n",
    "\n",
    "Choosing the model that achieved the best performance on the validation set (instead of the training set) avoids selecting a model that has overfit. However, when comparing many models based on validation score, we might select a model that only worked best for the particular random choice of training and validation data. To make a better-informed decision, we can use **cross-validation** techniques.\n",
    "\n",
    "A basic cross-validation technique is **$k$-fold cross-validation**. Here, the data set is first split up into only two subsets: a *training* set and a *test* set. The training set is partitioned into **$k$ equally-sized folds**. When training a model, we choose one fold for *validation* and the other $k - 1$ folds for *training*. By training a certain model configuration with all $k$ different choices for training and validation and averaging the validation scores, we can effectively select the best model configuration in a more reliable and robust manner. The figure below illustrates the process of $k$-fold cross-validation ([source](https://scikit-learn.org/stable/modules/cross_validation.html)).\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" style=\"max-width: 500px; margin: auto; padding: 2em;\"/>\n",
    "\n",
    "To add support for $k$-fold cross validation to our `PokemonDataset`, we'll first use `train_test_split()` to split our dataset into train+val and test set. Then, with [`numpy.array_split()`](https://numpy.org/doc/stable/reference/generated/numpy.array_split.html), we can split the train+val set into $k$ folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e7c5f90-ddbb-402e-97e8-c4bd7110b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefve\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_trainval, df_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "k=5\n",
    "folds =  np.array_split(df_trainval,k) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb64f9-be7e-4001-930a-3317a6e35658",
   "metadata": {},
   "source": [
    "Now, we can choose which fold to use for validation. The other folds should be concatenated to create the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44ca9e13-2141-415e-a8dc-01887dd24785",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fold = 0\n",
    "df_val = folds[val_fold]\n",
    "train_folds = [fold for i, fold in enumerate(folds) if i != val_fold]\n",
    "df_train = pd.concat(train_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21c21e-ec62-4d78-9870-657b83fa0109",
   "metadata": {},
   "source": [
    "Plugging this into our `SimpsonsDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2f3d928-ed42-452a-b02a-827ddb9445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    def __init__(self,datapath, subset, k=5, val_fold = 0, transform=None):\n",
    "        data_path= Path(datapath)\n",
    "        df = pd.DataFrame([\n",
    "        {\n",
    "    \n",
    "            'image': p,\n",
    "            'label': p.parent.name\n",
    "        }\n",
    "        \n",
    "        \n",
    "        for p in data_path.glob('data/*/*')\n",
    "        ])\n",
    "        label_to_int = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
    "\n",
    "        #Train, test and val\n",
    "        df_trainval, df_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "        # Cross Validation\n",
    "        folds = np.array_split(df_trainval, k)\n",
    "        df_val = folds[val_fold]\n",
    "        train_folds = [fold for i, fold in enumerate(folds) if i != val_fold]\n",
    "        df_train = pd.concat(train_folds)\n",
    "        \n",
    "        self.df = df\n",
    "        self.label_to_int = label_to_int\n",
    "        self.transform = transform\n",
    "\n",
    "        if subset == 'train':\n",
    "            self.df = df_train.reset_index()\n",
    "        elif subset == 'val':\n",
    "            self.df = df_val.reset_index()\n",
    "        elif subset == 'test':\n",
    "            self.df = df_val.reset_index()\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['image']\n",
    "        label = row['label']\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb27d3b-93a7-45ad-a651-25c9d21b13b6",
   "metadata": {},
   "source": [
    "Now we can create a training set, validation set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "333df9a0-1b73-4ece-a620-5c252714fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Create transforms\n",
    "train_tfms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.RandomResizedCrop(224, antialias=True),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Bij de validatie set geen randomness\n",
    "val_tfms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(224, antialias=True),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "ds_train = FlowersDataset('../data/Flowers', 'train', transform=train_tfms)\n",
    "ds_val = FlowersDataset('../data/Flowers', 'val', transform=val_tfms)\n",
    "ds_test = FlowersDataset('../data/Flowers', 'test', transform=val_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb8f52-a3ae-4ad0-aeda-31906cc7502a",
   "metadata": {},
   "source": [
    "> ⚠️ **Don't use random transforms for validation or test set!**\n",
    ">\n",
    "> To avoid making the validation and test evaluations irreproducible, you should not use randomness in the transforms that you'll use for the validation and test set.\n",
    "\n",
    "Hooray! 🙌 We now have a full-fledged PyTorch `Dataset` with support for $k$-fold cross-validation! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354dc290-0d9a-4d1d-b427-926424a92131",
   "metadata": {},
   "source": [
    "## 2.2 PyTorch data loader\n",
    "\n",
    "A PyTorch `Dataset` allows us to easily get images (and labels) from our dataset. When training a neural network, however, we can save training time by training with **batches** of data, instead of passing the images through the network one-by-one.\n",
    "\n",
    "Batching your data is the task of the [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Given a PyTorch `Dataset` that returns a tuple with an image tensor and an integer label, it is very simple to create a `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33459067-70bd-45e0-a3a5-c393ae9f0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c43cee-01c4-4c0a-89c3-0ad923165ed6",
   "metadata": {},
   "source": [
    "A `DataLoader` is an iterable, where each iteration item is a batch of data samples. As with any iterable in Python, you can iterate over a `DataLoader` using a `for` loop. Let's inspect what's inside an iteration item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a45a4fd8-8a2d-47ad-bed7-a83742ec1633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 2.1290,  2.1462,  2.1633,  ...,  0.3994,  0.5193,  0.6049],\n",
       "           [ 2.0777,  2.1119,  2.1462,  ...,  0.4679,  0.5364,  0.5878],\n",
       "           [ 1.9920,  2.0263,  2.0948,  ...,  0.5536,  0.5878,  0.5364],\n",
       "           ...,\n",
       "           [-1.9467, -1.9467, -1.9638,  ..., -1.3130, -1.2617, -1.2445],\n",
       "           [-1.9467, -1.9467, -1.9467,  ..., -1.3473, -1.3130, -1.2959],\n",
       "           [-1.9467, -1.9467, -1.9467,  ..., -1.3644, -1.3302, -1.3130]],\n",
       " \n",
       "          [[ 1.5357,  1.4832,  1.4132,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 1.5182,  1.5007,  1.4657,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 1.5007,  1.5182,  1.5357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-1.8256, -1.8256, -1.8431,  ..., -1.4930, -1.5455, -1.5805],\n",
       "           [-1.8256, -1.8256, -1.8256,  ..., -1.4580, -1.5105, -1.5455],\n",
       "           [-1.8256, -1.8256, -1.8256,  ..., -1.4405, -1.4930, -1.5280]],\n",
       " \n",
       "          [[ 2.3611,  2.2740,  2.1868,  ..., -1.7173, -1.6476, -1.5779],\n",
       "           [ 2.3437,  2.2914,  2.2391,  ..., -1.6824, -1.6650, -1.6476],\n",
       "           [ 2.2914,  2.3088,  2.3088,  ..., -1.6302, -1.6824, -1.7522],\n",
       "           ...,\n",
       "           [-1.6127, -1.6127, -1.6302,  ..., -1.5430, -1.4733, -1.4384],\n",
       "           [-1.6127, -1.6127, -1.6127,  ..., -1.5953, -1.5430, -1.5081],\n",
       "           [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.5779, -1.5430]]],\n",
       " \n",
       " \n",
       "         [[[-0.4397, -0.4568, -0.5082,  ..., -1.5185, -1.4843, -1.4843],\n",
       "           [-0.4397, -0.4568, -0.5082,  ..., -1.5185, -1.4843, -1.4843],\n",
       "           [-0.4226, -0.4397, -0.4911,  ..., -1.5014, -1.4672, -1.4672],\n",
       "           ...,\n",
       "           [ 1.6153,  1.6153,  1.6324,  ..., -1.1247, -1.1418, -1.1418],\n",
       "           [ 1.6153,  1.6153,  1.6324,  ..., -1.1418, -1.1589, -1.1589],\n",
       "           [ 1.6153,  1.6153,  1.6324,  ..., -1.1418, -1.1589, -1.1589]],\n",
       " \n",
       "          [[ 0.1877,  0.1702,  0.1001,  ..., -0.6877, -0.6702, -0.6702],\n",
       "           [ 0.1877,  0.1702,  0.1001,  ..., -0.6877, -0.6702, -0.6702],\n",
       "           [ 0.2052,  0.1877,  0.1176,  ..., -0.6702, -0.6527, -0.6527],\n",
       "           ...,\n",
       "           [ 0.5553,  0.5728,  0.6604,  ..., -0.0924, -0.1099, -0.1099],\n",
       "           [ 0.5378,  0.5553,  0.6429,  ..., -0.1099, -0.1275, -0.1275],\n",
       "           [ 0.5378,  0.5553,  0.6429,  ..., -0.1099, -0.1275, -0.1275]],\n",
       " \n",
       "          [[ 0.3742,  0.3568,  0.2522,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 0.3742,  0.3568,  0.2522,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 0.3916,  0.3742,  0.2696,  ..., -1.8044, -1.7870, -1.7870],\n",
       "           ...,\n",
       "           [-1.6999, -1.7173, -1.7696,  ..., -0.7761, -0.7936, -0.7936],\n",
       "           [-1.6824, -1.6999, -1.7522,  ..., -0.7936, -0.8110, -0.8110],\n",
       "           [-1.6824, -1.6999, -1.7522,  ..., -0.7936, -0.8110, -0.8110]]],\n",
       " \n",
       " \n",
       "         [[[-1.3302, -1.3302, -1.3130,  ..., -1.8953, -1.9124, -1.9124],\n",
       "           [-1.3302, -1.3302, -1.3130,  ..., -1.8953, -1.9124, -1.9124],\n",
       "           [-1.3302, -1.3302, -1.3130,  ..., -1.8782, -1.8953, -1.9124],\n",
       "           ...,\n",
       "           [-1.7240, -1.7412, -1.7583,  ..., -1.7069, -1.7069, -1.7069],\n",
       "           [-1.7412, -1.7412, -1.7583,  ..., -1.7069, -1.7069, -1.7069],\n",
       "           [-1.7412, -1.7412, -1.7583,  ..., -1.7069, -1.7069, -1.7069]],\n",
       " \n",
       "          [[-1.0728, -1.0728, -1.0553,  ..., -1.7731, -1.7906, -1.7906],\n",
       "           [-1.0728, -1.0728, -1.0553,  ..., -1.7731, -1.7906, -1.7906],\n",
       "           [-1.0903, -1.0903, -1.0728,  ..., -1.7556, -1.7731, -1.7906],\n",
       "           ...,\n",
       "           [-1.5980, -1.6155, -1.6331,  ..., -1.5280, -1.5280, -1.5280],\n",
       "           [-1.6155, -1.6155, -1.6331,  ..., -1.5280, -1.5280, -1.5280],\n",
       "           [-1.6155, -1.6155, -1.6331,  ..., -1.5280, -1.5280, -1.5280]],\n",
       " \n",
       "          [[-1.1421, -1.1421, -1.1247,  ..., -1.5953, -1.6127, -1.6127],\n",
       "           [-1.1421, -1.1421, -1.1247,  ..., -1.5953, -1.6127, -1.6127],\n",
       "           [-1.1247, -1.1247, -1.1073,  ..., -1.5779, -1.5953, -1.6127],\n",
       "           ...,\n",
       "           [-1.4559, -1.4733, -1.4907,  ..., -1.4036, -1.4036, -1.4036],\n",
       "           [-1.4733, -1.4733, -1.4907,  ..., -1.4036, -1.4036, -1.4036],\n",
       "           [-1.4733, -1.4733, -1.4907,  ..., -1.4036, -1.4036, -1.4036]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5125,  1.5297,  1.5468,  ...,  2.2489,  2.2489,  2.2489],\n",
       "           [ 1.5125,  1.5297,  1.5297,  ...,  2.2489,  2.2489,  2.2489],\n",
       "           [ 1.5125,  1.5125,  1.5125,  ...,  2.2489,  2.2489,  2.2489],\n",
       "           ...,\n",
       "           [ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[ 1.6758,  1.6933,  1.7108,  ...,  2.4286,  2.4286,  2.4286],\n",
       "           [ 1.6758,  1.6933,  1.6933,  ...,  2.4286,  2.4286,  2.4286],\n",
       "           [ 1.6758,  1.6758,  1.6583,  ...,  2.4286,  2.4286,  2.4286],\n",
       "           ...,\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[ 1.8557,  1.8731,  1.8905,  ...,  2.6400,  2.6400,  2.6400],\n",
       "           [ 1.8557,  1.8557,  1.8557,  ...,  2.6400,  2.6400,  2.6400],\n",
       "           [ 1.8557,  1.8383,  1.8034,  ...,  2.6400,  2.6400,  2.6400],\n",
       "           ...,\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         [[[-1.3302, -1.1760, -1.0219,  ...,  0.6906,  0.4337,  0.1768],\n",
       "           [-1.2274, -1.0219, -0.7650,  ...,  0.8276,  0.3823, -0.0287],\n",
       "           [-1.0048, -0.7308, -0.3712,  ...,  0.7591,  0.4166,  0.0912],\n",
       "           ...,\n",
       "           [ 1.1358,  0.6221,  0.0912,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [ 0.1597, -0.0116, -0.1486,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-0.4568, -0.1657,  0.1254,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-1.5280, -1.3529, -1.1604,  ..., -0.1800, -0.4251, -0.6527],\n",
       "           [-1.4755, -1.2829, -1.0203,  ..., -0.0399, -0.4601, -0.8627],\n",
       "           [-1.3529, -1.0728, -0.7227,  ..., -0.1625, -0.4776, -0.7752],\n",
       "           ...,\n",
       "           [-0.5301, -1.0378, -1.5630,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-1.5105, -1.6506, -1.7731,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -1.7556, -1.4405,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.4733, -1.2816, -1.0724,  ...,  1.7860,  1.4897,  1.1934],\n",
       "           [-1.2816, -1.0027, -0.6890,  ...,  1.9254,  1.4200,  0.9668],\n",
       "           [-0.9504, -0.5670, -0.1312,  ...,  1.8557,  1.4548,  1.0888],\n",
       "           ...,\n",
       "           [ 2.1520,  1.6640,  1.1062,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 1.4025,  1.1411,  0.8797,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 0.8622,  1.0017,  1.1585,  ..., -1.8044, -1.8044, -1.8044]]]]),\n",
       " ('Orchid',\n",
       "  'Orchid',\n",
       "  'Lily',\n",
       "  'Poppy',\n",
       "  'Aster',\n",
       "  'Daisy',\n",
       "  'Lily',\n",
       "  'Rose',\n",
       "  'Daisy',\n",
       "  'Aster')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in dl_train:\n",
    "    break\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6e06e-ffb3-4b12-b79b-16137453115a",
   "metadata": {},
   "source": [
    "Our `DataLoader` composes the first batch with the samples at index $0,\\ldots, 9$, the second batch $10,\\ldots,19$ and so on. To compose the batches with random samples, you can pass `shuffle=True` to the `DataLoader` constructor.\n",
    "\n",
    "> ⚠️ **Only shuffle the training set!**\n",
    ">\n",
    "> It is considered bad practice to shuffle any of your evaluation sets, as this might make your evaluations irreproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4c7cc79-1ce6-4315-a2a0-871668fc42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6e4c6-3cdc-4be5-a883-2eefb76219e0",
   "metadata": {},
   "source": [
    "When taking a look at the implementation of `__getitem__()` in our `PokemonDataset`, you'll see that it involves loading an image from disk with `read_image()` and applying image transforms. Both of these steps may block the computing process for a while. If we'd sequentially run `__getitem__()` for the 10 indices in our batch, it might take some time before the batch is created. Setting the argument `num_workers` as a positive integer will turn on **multi-process data loading** with the specified number of loader worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "737e4fb7-9e7e-4f43-833b-65847eb79eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=10, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0f976-19db-42ee-8f35-07cdc1d7e781",
   "metadata": {},
   "source": [
    "In summary, the following code creates our train, validation and test datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "463f2957-4ac5-4209-9dc1-594fde0d4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "dl_train = DataLoader(ds_train, batch_size=10, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=10, shuffle=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc0e5a5-b2fc-4c0a-9ed0-6a0e171bdf1c",
   "metadata": {},
   "source": [
    "And with this, we have covered everything on the data side! 💪 Our data is now ready to be passed into a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f4cccdf-820d-4540-abba-5ea4d3bf1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
